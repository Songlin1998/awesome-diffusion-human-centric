# Awesome-Diffusion-Human-Centric  

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

---

## üìë Table
- [üßëüèª Face](#-face)
- [üèÉ Body](#-body)

--- 

## üßëüèª Face

### Face Generation
* **DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation** | *CVPR '22* | [[Paper]](https://arxiv.org/abs/2110.02711)
* **Diffusion Models Already Have a Semantic Latent Space** | *ICLR '22* | [[Paper]](https://arxiv.org/abs/2210.10960)
* **Collaborative Diffusion for Multi-Modal Face Generation and Editing** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2304.10530)
* **DiffusionRig: Learning Personalized Priors for Facial Appearance Editing** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2304.06711)
* **Diffusion Autoencoders: Toward a Meaningful and Decodable Representation** | *CVPR '22* | [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.html)
* **FaceComposer: A Unified Model for Versatile Facial Content Creation** | *NeurIPS '23* | [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2b4caf39e645680f826ae0a9e7ae9402-Abstract-Conference.html)
* **Diffusion-Driven GAN Inversion for Multi-Modal Face Image Generation** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2405.04356)
* **DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2403.19235)
* **DiffTV: Identity-Preserved Thermal-to-Visible Face Translation via Feature Alignment and Dual-Stage Conditions** | *ACM MM '24* | [[Paper]](https://dl.acm.org/doi/10.1145/3664647.3680635)
* **Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2212.08698)
* **Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2401.01207)
* **Arc2Face: A Foundation Model for ID-Consistent Human Faces** | *ECCV '24* | [[Paper]](https://arxiv.org/abs/2403.11641)
* **Z-Magic: Zero-Shot Multiple Attributes Guided Image Creator** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.12124)

### Face Editing
* **DiffFace: Diffusion-based Face Swapping with Facial Guidance** | *PR '25* | [[Paper]](https://arxiv.org/abs/2212.13344)
* **DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion** | *CVPR '23* | [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf)
* **Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control** | *ECCV '24* | [[Paper]](https://arxiv.org/abs/2405.12970)
* **FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple Reference Images** | *NeurIPS '24* | [[Paper]](https://arxiv.org/abs/2410.22771)
* **DynamicFace: High-Quality and Consistent Face Swapping for Image and Video Using Composable 3D Facial Priors** | *ICCV '25* | [[Paper]](https://arxiv.org/abs/2501.08553)
* **VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2412.11279)
* **CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation** | *ICCV '25* | [[Paper]](https://arxiv.org/abs/2507.02691)
* **Toward Tiny and High-Quality Facial Makeup with Data Amplify Learning** | *ECCV '24* | [[Paper]](https://arxiv.org/abs/2403.15033)
* **StableMakeup: When Real-World Makeup Transfer Meets Diffusion Model** | *SIGGRAPH '25* | [[Paper]](https://arxiv.org/abs/2403.07764)
* **SHMT: Self-Supervised Hierarchical Makeup Transfer via Latent Diffusion Models** | *NeurIPS '24* | [[Paper]](https://arxiv.org/abs/2412.11058)
* **DiffAM: Diffusion-Based Adversarial Makeup Transfer for Facial Privacy Protection** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2405.09882)
* **MAD: Makeup All-in-One with Cross-Domain Diffusion Model** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2504.02545)
* **FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2508.05069)
* **Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2507.07591)
* **DiffLocks: Generating 3D Hair from a Single Image Using Diffusion Models** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2505.06166)
* **HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation** | *ACM MM '25* | [[Paper]](https://arxiv.org/abs/2507.12758)
* **HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion** | *NeurIPS '24* | [[Paper]](https://arxiv.org/abs/2410.21789)
* **Stable-Hair: Real-World Hair Transfer via Diffusion Model** | *AAAI '25* | [[Paper]](https://arxiv.org/abs/2507.07591)
* **Face Aging via Diffusion-Based Editing** | *BMVC '23* | [[Paper]](https://arxiv.org/abs/2309.11321)
* **Identity-Preserving Aging of Face Images via Latent Diffusion Models** | *IJCB '23* | [[Paper]](https://arxiv.org/abs/2307.08585)
* **TimeBooth: Disentangled Facial Invariant Representation for Diverse and Personalized Face Aging** | *ICCV '25* | [[Paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_TimeBooth_Disentangled_Facial_Invariant_Representation_for_Diverse_and_Personalized_Face_ICCV_2025_paper.pdf)
* **UV-IDM: Identity-Conditioned Latent Diffusion Model for Face UV-Texture Generation** | *CVPR '24* | [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Li_UV-IDM_Identity-Conditioned_Latent_Diffusion_Model_for_Face_UV-Texture_Generation_CVPR_2024_paper.html)
* **FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.17197)
* **UVMap-ID: A Controllable and Personalized UV Map Generative Model** | *ACM MM '24* | [[Paper]](https://arxiv.org/abs/2404.14568)
* **PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance** | *NeurIPS '23* | [[Paper]](https://arxiv.org/abs/2309.10810)
* **DR2: Diffusion-Based Robust Degradation Remover for Blind Face Restoration** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2303.06885)
* **DiffBFR: Bootstrapping Diffusion Model for Blind Face Restoration** | *ACM MM '23* | [[Paper]](https://arxiv.org/abs/2305.04517)
* **Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond** | *ICCV '23* | [[Paper]](https://arxiv.org/abs/2307.08996)
* **Diffuse and Restore: A Region-Adaptive Diffusion Model for Identity-Preserving Blind Face Restoration** | *WACV '24* | [[Paper]](https://openaccess.thecvf.com/content/WACV2024/papers/Suin_Diffuse_and_Restore_A_Region-Adaptive_Diffusion_Model_for_Identity-Preserving_Blind_WACV_2024_paper.pdf)
* **DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance** | *ICCV '25* | [[Paper]](https://arxiv.org/abs/2507.13797)
* **OSDFace: One-Step Diffusion Model for Face Restoration** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2411.17163)
* **Toward Real-World Blind Face Restoration with Generative Diffusion Prior** | *TCSVT '24* | [[Paper]](https://arxiv.org/abs/2312.15736)
* **Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model** | *ICLR '25* | [[Paper]](https://arxiv.org/abs/2410.04161)

### Face Animation
* **Dae-talker: High fidelity speech-driven talking face generation with diffusion autoencoder** | *ACM MM '23* | [[Paper]](https://arxiv.org/abs/2303.17550)
* **Fd2talk: Towards generalized talking head generation with facial decoupled diffusion model** | *ACM MM '24* | [[Paper]](https://arxiv.org/abs/2408.09384)
* **Gaia: Zero-shot talking avatar generation** | *ICLR '23* | [[Paper]](https://arxiv.org/abs/2311.15230)
* **Mfr-net: Multi-faceted responsive listening head generation via denoising diffusion model** | *ACM MM '23* | [[Paper]](https://arxiv.org/abs/2308.16635)
* **Landmark-guided diffusion model for high-fidelity and temporally coherent talking head generation** | *ICME '24* | [[Paper]](https://arxiv.org/abs/2408.01732)
* **Laughing matters: Introducing laughing-face generation using diffusion models** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2305.08854)
* **Multimodal fusion for talking face generation utilizing speech-related facial action units** | *TOMM '24* | [[Paper]](https://mftfg-au.github.io/Multimodal_Fusion/)
* **Vasa-1: Lifelike audio-driven talking faces generated in real time** | *NeurIPS '24* | [[Paper]](https://arxiv.org/abs/2404.10667)
* **Diffused heads: Diffusion models beat gans on talking-face generation** | *WACV '24* | [[Paper]](https://openaccess.thecvf.com/content/WACV2024/papers/Kligvasser_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.pdf)
* **Talking head generation with probabilistic audio-to-visual diffusion priors** | *ICCV '23* | [[Paper]](https://arxiv.org/abs/2212.04248)
* **Audio-driven talking head video generation with diffusion model** | *ICASSP '23* | [[Paper]](https://ieeexplore.ieee.org/abstract/document/10094937)
* **Diff2lip: Audio conditioned diffusion models for lip-synchronization** | *WACV '24* | [[Paper]](https://arxiv.org/abs/2308.09716)
* **Diffdub: Person-generic visual dubbing using inpainting renderer with diffusion auto-encoder** | *ICASSP '24* | [[Paper]](https://arxiv.org/abs/2311.01811)
* **Difftalk: Crafting diffusion models for generalized audio-driven portraits animation** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2301.03786)
* **Float: Generative motion latent flow matching for audio-driven talking portrait** | *ICCV '25* | [[Paper]](https://arxiv.org/abs/2412.01064)
* **Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions** | *ECCV '25* | [[Paper]](https://arxiv.org/abs/2402.17485)
* **Eat-face: Emotion-controllable audio-driven talking face generation via diffusion model** | *FG '24* | [[Paper]](https://www.semanticscholar.org/paper/EAT-Face%3A-Emotion-Controllable-Audio-Driven-Talking-Wang-Jia/7b142d08b1319ab1d759083ff65b153400c907cb)
* **Emotalker: Emotionally editable talking face generation via diffusion model** | *ICASSP '24* | [[Paper]](https://arxiv.org/abs/2401.08049)
* **Emotivetalk: Expressive talking head generation through audio information decoupling and emotional video diffusion** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2411.16726)
* **Facechain-imagineid: Freely crafting high-fidelity diverse talking faces from disentangled audio** | *CVPR '24* | [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FaceChain-ImagineID_Freely_Crafting_High-Fidelity_Diverse_Talking_Faces_from_Disentangled_Audio_CVPR_2024_paper.pdf)
* **Hallo: Hierarchical audio-driven visual synthesis for portrait image animation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2406.08801)
* **Loopy: Taming audio-driven portrait avatar with long-term motion dependency** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2409.02634)
* **Realistic speech-to-face generation with speech-conditioned latent diffusion model with face prior** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2310.03363)
* **Svp: Style-enhanced vivid portrait talking head diffusion model** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2409.03270)
* **Control-talker: A rapid-customization talking head generation method for multi-condition control and high-texture enhancement** | *ACM MM '24* | [[Paper]](https://dl.acm.org/doi/10.1145/3664647.3681644)
* **Keyface: Expressive audio-driven facial animation for long sequences via keyframe interpolation** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.01715)
* **Stableavatar: Infinite-length audio-driven avatar video generation** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2508.08248)
* **Sonic: Shifting focus to global audio perception in portrait animation** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2411.16331)
* **Leo: Generative latent image animator for human video synthesis** | *IJCV '24* | [[Paper]](https://arxiv.org/abs/2305.03989)
* **Diffusionact: Controllable diffusion autoencoder for one-shot face reenactment** | *FG '25* | [[Paper]](https://arxiv.org/abs/2403.17217)
* **Moditalker: Motion-disentangled diffusion model for high-fidelity talking head generation** | *AAAI '25* | [[Paper]](https://arxiv.org/abs/2403.19144)
* **Anchored diffusion for video face reenactment** | *WACV '25* | [[Paper]](https://openaccess.thecvf.com/content/WACV2025/papers/Kligvasser_Anchored_Diffusion_for_Video_Face_Reenactment_WACV_2025_paper.pdf)
* **Fantasyportrait: Enhancing multi-character portrait animation with expression-augmented diffusion transformers** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2507.12956)
* **X-portrait: Expressive portrait animation with hierarchical motion attention** | *SIGGRAPH '24* | [[Paper]](https://arxiv.org/abs/2403.15931)
* **Megactor: Harness the power of raw video for vivid portrait animation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2405.20851)
* **Mimaface: Face animation via motion-identity modulated appearance feature learning** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2409.15179)
* **Fantasytalking: Enhancing multi-character portrait animation with expression-augmented diffusion transformers** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2504.04842)
* **Text-based talking video editing with cascaded conditional diffusion** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2407.14841)
* **Text-driven synchronized diffusion video and audio talking head generation** | *MIPR '24* | [[Paper]](https://www.albany.edu/faculty/mchang2/files/2024-08_MIPR_VidAudio_Gen.pdf)
* **Omnitalker: One-shot real-time text-driven talking audio-video generation with multimodal style mimicking** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2504.02433)
* **Diffusionavatars: Deferred diffusion for high-fidelity 3d head avatars** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2311.18635)
* **Expavatar: High-fidelity avatar generation of unseen expressions with 3d face priors** | *TOMM '24* | [[Paper]](https://www.researchgate.net/publication/384948919_ExpAvatar_High-Fidelity_Avatar_Generation_of_Unseen_Expressions_with_3D_Face_Priors)
* **Zero-1-to-a: Zero-shot one image to animatable head avatars using video diffusion** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.15851)
* **Cap4d: Creating animatable 4d portrait avatars with morphable multi-view diffusion models** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2412.12093)
* **Facecraft4d: Animated 3d facial avatar generation from a single image** | *ICCV '25* | [[Paper]](https://arxiv.org/abs/2504.15179)
* **Facecomposer: A unified model for versatile facial content creation** | *NeurIPS '24* | [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2b4caf39e645680f826ae0a9e7ae9402-Abstract-Conference.html)
* **Multimodal-driven talking face generation via a unified diffusion-based generator** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2305.02594)
* **Posetalk: Text-and-audio-based pose control and motion refinement for one-shot talking head generation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2409.02657)
* **Ditto: Motion-space diffusion for controllable realtime talking head synthesis** | *ACM MM '25* | [[Paper]](https://arxiv.org/abs/2411.19509)
* **Magic-talk: Motion-aware audio-driven talking face generation with customizable identity control** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2510.22810)
* **Long-term talkingface generation via motion-prior conditional diffusion model** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2502.09533)
* **Consistentavatar: Learning to diffuse fully consistent talking head avatar with temporal guidance** | *ACM MM '24* | [[Paper]](https://arxiv.org/abs/2411.15436)
* **Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation** | *SIGGRAPHAsia '24* | [[Paper]](https://dl.acm.org/doi/10.1145/3680528.3687587)
* **One-shot pose-driving face animation platform** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2407.08949)
* **Stable video portraits** | *ECCV '25* | [[Paper]](https://arxiv.org/abs/2409.18083)
* **Style-preserving lip sync via audio-aware style reference** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2408.05412)
* **V-express: Conditional dropout for progressive training of portrait video generation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2406.02511)
* **Avatarstudio: Text-driven editing of 3d dynamic human head avatars** | *TOG '23* | [[Paper]](https://arxiv.org/abs/2306.00547)
* **Instruct-video2avatar: Video-to-avatar generation with instructions** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2306.02903)
* **Follow-your-emoji-faster: Towards efficient, fine-controllable, and expressive freestyle portrait animation** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2509.16630)

## üèÉ Body

### Body Generation
* **Paper Title Here** | *NeurIPS '23* | [[Paper]](ÈìæÊé•)

### Virtual Try-On
* **Controllable Accelerated Virtual Try-On with Diffusion Model** | *CVPR '24* | [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_CAT-DM_Controllable_Accelerated_Virtual_Try-on_with_Diffusion_Model_CVPR_2024_paper.pdf)
* **Time-Efficient and Identity-Consistent Virtual Try-On Using a Variant of Altered Diffusion Models** | *ECCV '24* | [[Paper]](https://arxiv.org/abs/2403.07371)
* **Parser-Free Virtual Try-On via Diffusion Model** | *ICASSP '24* | [[Paper]](https://ieeexplore.ieee.org/abstract/document/10446540)
* **Virtual Try-On with Pose-Garment Keypoints Guided Inpainting** | *ICCV '23* | [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Virtual_Try-On_with_Pose-Garment_Keypoints_Guided_Inpainting_ICCV_2023_paper.pdf)
* **Deep Text-Driven Virtual Try-On via Hybrid Attention Learning** | *ICASSP '25* | [[Paper]](https://arxiv.org/abs/2401.13795)
* **Product-Level Try-On: Characteristics-Preserving Try-On with Realistic Clothes Shading and Wrinkles** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2401.11239)
* **Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2401.13795)
* **Latent Diffusion Textual-Inversion Enhanced Virtual Try-On** | *ACM MM '23* | [[Paper]](https://arxiv.org/abs/2305.13501)
* **OutfitAnyone: Ultra-High Quality Virtual Try-On for Any Clothing and Any Person** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2407.16224)
* **Image-Timestep Adaptive Masked Diffusion Transformer Framework for Image Based Virtual Try-On** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.20418)
* **Shining Yourself: High-Fidelity Ornaments Virtual Try-On with Diffusion Model** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.16065)
* **A Size-Controllable Virtual Try-On Approach for Informed Decision-Making** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2408.02803)
* **Enhancing Consistency in Virtual Try-On: A Novel Diffusion-Based Approach** | *Image and Vision Computing '24* | [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0262885624002014)
* **Street TryOn: Learning In-the-Wild Virtual Try-On from Unpaired Person Images** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2311.16094)
* **Virtual Try-On for Arbitrary Hand Pose Guided by Hand Priors Embedding** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2408.12340)
* **Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2405.18326)
* **Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2407.15111)
* **Image-based 3D Virtual Try-On with Image Prompt Adapter** | *AAAI '25* | [[Paper]](https://arxiv.org/abs/2501.15616)
* **Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow** | *ACM MM '23* | [[Paper]](https://arxiv.org/abs/2308.06101)
* **Video Virtual Try-On using Diffusion Models** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2405.11794)
* **Garment-Centric Generation via Stable Diffusion** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2403.10783)
* **A Unified Model for Editing Human Images in the Wild** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2312.14985)
* **Multi-Modal Multi Reference Control for High-Quality Fashion Generation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2405.00448)
* **Photorealistic Virtual Try On from Unconstrained Designs** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2312.04534)
* **Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2312.14238)
* **A Two-Stage Personalized Virtual Try-On Framework with Shape Control and Texture Guidance** | *IEEE TMM '24* | [[Paper]](https://arxiv.org/abs/2312.15480)
* **Animated clothing fitting: Semantic-visual fusion for video customization virtual try-on with diffusion models** | *ICECAI '24* | [[Paper]](https://ieeexplore.ieee.org/abstract/document/10674974)
* **Boosting In-the-Wild Virtual Try-On via Mask-Free Pseudo Data Training** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2408.06047)
* **A 3D Virtual Try-On Method with Global-Local Alignment and Diffusion Model** | *ICASSP '24* | [[Paper]](https://ieeexplore.ieee.org/abstract/document/10447311)
* **Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2505.16977)
* **High-Fidelity Virtual Try On from Any Viewing Direction** | *CVPR '25* | [[Paper]](https://arxiv.org/abs/2503.12165)
* **Video Diffusion Model for Virtual Try-On** | *SIGGRAPH Asia '24* | [[Paper]](https://arxiv.org/abs/2411.00225)
* **Preserving Garment Details in Video Virtual Try-On** | *ACM MM '24* | [[Paper]](https://dl.acm.org/doi/10.1145/3664647.3680701)
* **Excavating Spatial Temporal Tunnels for High-Quality Virtual Try-On in Videos** | *ACM MM '24* | [[Paper]](https://arxiv.org/abs/2404.17571)
* **Fast and Consistent Video Virtual Try-On with Diffusion Models** | *AAAI '25* | [[Paper]](https://arxiv.org/abs/2412.10178)
* **Video Virtual Try On in the Wild via Image-Based Controlled Diffusion Models** | *ECCV '24* | [[Paper]](https://arxiv.org/abs/2407.10625)

### Human Animation
* **Dance Any Beat: Blending Beats with Visuals in Dance Video Generation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2405.09266)
* **Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2404.01862)
* **DiffTED: One-Shot Audio-Driven TED Talk Video Generation with Diffusion-Based Co-Speech Gestures** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2409.07649)
* **IMAC: Implicit Motion-Audio Coupling for Co-Speech Gesture Video Generation** | *ICLR '25* | [[Paper]](https://openreview.net/pdf?id=kmhNK0fs8c)
* **Conditional Image-to-Video Generation with Latent Flow Diffusion Models** | *CVPR '23* | [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf)
* **Make-A-Story: Visual Memory Conditioned Consistent Story Generation** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2211.13319)
* **DynamiCrafter: Animating Open-Domain Images with Video Diffusion Priors** | *ECCV '25* | [[Paper]](https://arxiv.org/abs/2310.12190)
* **VideoBooth: Diffusion-Based Video Generation with Image Prompts** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2312.00777)
* **LivePhoto: Real Image Animation with Text-Guided Motion Control** | *ECCV '25* | [[Paper]](https://arxiv.org/abs/2312.02928)
* **Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling** | *SIGGRAPH '24* | [[Paper]](https://arxiv.org/abs/2401.15977)
* **Boosting Consistency in Story Visualization with Rich-Contextual Conditional Diffusion Models** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2407.02482)
* **AnyTop: Character Animation Diffusion with Any Topology** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2502.17327)
* **DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion** | *ICCV '23* | [[Paper]](https://arxiv.org/abs/2304.06025)
* **DisCo: Disentangled Control for Realistic Human Dance Generation** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2307.00040)
* **MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-Aware Diffusion** | *ICML '23* | [[Paper]](https://arxiv.org/abs/2311.12052)
* **Make-Your-Anchor: A Diffusion-Based 2D Avatar Generation Framework** | *CVPR '23* | [[Paper]](https://arxiv.org/abs/2403.16510)
* **Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2311.17117)
* **MagicAnimate: Temporally Consistent Human Image Animation Using Diffusion Model** | *CVPR '24* | [[Paper]](https://arxiv.org/abs/2311.16498)
* **CHAMP: Controllable and Consistent Human Image Animation with 3D Parametric Guidance** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2403.14781)
* **Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control** | *arXiv '24* | [[Paper]](https://arxiv.org/html/2406.03035v1)
* **HumanGIF: Single-View Human Diffusion with Generative Prior** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2502.12080)
* **DreamHA: Towards High-Quality Human Animation with Image-to-Video Diffusion Models** | *ICASSP '25* | [[Paper]](https://ieeexplore.ieee.org/abstract/document/10890226)
* **StableAnimator: High-Quality Identity-Preserving Human Image Animation** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2411.17697)
* **X-Dyna: Expressive Dynamic Human Image Animation** | *arXiv '25* | [[Paper]](https://arxiv.org/abs/2501.10021)
* **Motion-Conditioned Diffusion Model for Controllable Video Synthesis** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2304.14404)
* **DragNUWA: Fine-Grained Control in Video Generation by Integrating Text, Image, and Trajectory** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2308.08089)
* **Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation** | *SIGGRAPH Asia '24* | [[Paper]](https://arxiv.org/abs/2406.01900)
* **VideoComposer: Compositional Video Synthesis with Motion Controllability** | *NeurIPS '24* | [[Paper]](https://arxiv.org/abs/2306.02018)
* **MotionCtrl: A Unified and Flexible Motion Controller for Video Generation** | *SIGGRAPH '24* | [[Paper]](https://arxiv.org/abs/2312.03641)
* **SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models** | *ECCV '25* | [[Paper]](https://arxiv.org/abs/2311.16933)
* **Zero-Shot High-Fidelity and Pose-Controllable Character Animation** | *IJCAI '24* | [[Paper]](https://arxiv.org/abs/2404.13680)
* **MagicFight: Personalized Martial Arts Combat Video Generation** | *ACM MM '24* | [[Paper]](https://arxiv.org/abs/2601.02107)
* **Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance** | *TVCG '24* | [[Paper]](https://arxiv.org/abs/2306.00943)
* **Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation** | *arXiv '23* | [[Paper]](https://arxiv.org/abs/2307.06940)
* **TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model** | *SIGGRAPH Asia '24* | [[Paper]](https://arxiv.org/abs/2410.10696)
* **CyberHost: Taming Audio-Driven Avatar Diffusion Model with Region Codebook Attention** | *arXiv '24* | [[Paper]](https://arxiv.org/abs/2409.01876)

---

Licensed under [CC-BY-4.0](LICENSE).
